{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlowとは\n",
    "\n",
    "TensorFlowはニューラルネットワークを構築するためのフレームワークの一種です。\n",
    "\n",
    "\n",
    "以下のコードはバージョン1.13.0での動作を確認しています。\n",
    "\n",
    "\n",
    "簡単な計算\n",
    "簡単な計算をさせてみます。TensorFlowの基本的な流れです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "a = tf.constant(5)\n",
    "b = tf.constant(7)\n",
    "add = tf.add(a, b)\n",
    "sess = tf.Session()\n",
    "output = sess.run(add)\n",
    "print(output) # 12\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(5)\n",
    "b = tf.constant(7)\n",
    "add = tf.add(a, b)\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(add)\n",
    "    print(output) # 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比較のために、NumPyでも同じ計算をさせてみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a_n = np.array(5)\n",
    "b_n = np.array(7)\n",
    "output_n = np.add(a_n, b_n)\n",
    "print(output_n) # 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データフロープログラミング\n",
    "\n",
    "TensorFlowの仕組みを見ていきます。TensorFlowではデータフローグラフを考えます。\n",
    "\n",
    "\n",
    "簡単な計算の例では以下のようなデータフローグラフになります。\n",
    "\n",
    "グラフはノード（丸い部分）とエッジ（線の部分）で表されます。ノードはオペレーション（操作）を表し、エッジはTensor（多次元配列）を表します。\n",
    "\n",
    "\n",
    "今回の例では左側の灰色のノードでは定数を作るオペレーション（操作）、右の水色のノードでは足し算のオペレーション（操作）が行われています。エッジは整数値です。\n",
    "\n",
    "TensorFlowの手順\n",
    "TensorFlowの手順は以下のようになっています。\n",
    "\n",
    "\n",
    "データフローグラフを構築する\n",
    "データを入力して計算する\n",
    "\n",
    "簡単な計算の例でそれぞれを解説します。\n",
    "\n",
    "\n",
    "データフローグラフの構築\n",
    "サンプルコードの以下の部分でデータフローグラフを構築しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(5)\n",
    "b = tf.constant(7)\n",
    "add = tf.add(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定数aと定数bを定義し、それらを足し算するデータフローグラフです。NumPyであれば3行で計算まで行いますが、TensorFlowでは実際の計算までは進みません。\n",
    "\n",
    "\n",
    "それぞれをprintしてみても、エッジ（Tensor）の説明が返されるだけです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_4:0\", shape=(), dtype=int32)\n",
      "Tensor(\"Add_2:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(a) # Tensor(\"Const:0\", shape=(), dtype=int32)\n",
    "print(add) # Tensor(\"Add:0\", shape=(), dtype=int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データを入力して計算\n",
    "データフローグラフの計算を行う際にはセッションという概念が登場します。\n",
    "\n",
    "\n",
    "まずSessionオブジェクトを作成します。\n",
    "そして、sess.run()の中にエッジ（Tensor）を入れます。そうすることで、そのエッジ（Tensor）がどのような値を持つかが返ってきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "output = sess.run(add)\n",
    "print(output) # 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlowにおける値の扱い方\n",
    "TensorFlowが値を扱う上で独自の概念として、placeholderとValiableがあります。\n",
    "\n",
    "\n",
    "《placeholder》\n",
    "placeholderはデータフローグラフの構築時には値が決まっていないものに使います。最初は配列の形だけ定義しておいて後から値を入れて使う空箱のような存在です。学習ごとに違う値が入る入力データや正解データなどに用いられます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "#サンプルコードをplaceholderで書き換え\n",
    "c = tf.placeholder(tf.int32)\n",
    "d = tf.placeholder(tf.int32)\n",
    "add = tf.add(c, d)\n",
    "sess = tf.Session()\n",
    "output = sess.run(add, feed_dict={c:5, d:7})\n",
    "print(output) # 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "セッションを実行する際に引数feed_dictを使い、placeholderに入れる値を辞書型で指定します。ここを書き換えることで異なる計算が可能になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "output = sess.run(add, feed_dict={c:20, d:32})\n",
    "print(output) # 52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回の簡単な例では定数とplaceholderの違いは感じられませんが、ミニバッチ学習を行うような場合を想定すると、必要性がわかります。\n",
    "？よくわかりません"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlowにおける演算\n",
    "サンプルコードではtf.add()を使用していますが、NumPyなどと同様に+を使用することも可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "add = a + b \n",
    "# tf.add(a, b)に等しい"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "セッションの終了\n",
    "最後にセッションは終了させます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "セッションのインスタンス化から終了までに対して、with構文を使うことも可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "run() missing 1 required positional argument: 'fetches'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-45aa08c9c9b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# ここに計算の実行コードを入れていく\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: run() missing 1 required positional argument: 'fetches'"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run() # ここに計算の実行コードを入れていく"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ロジスティック回帰の実装\n",
    "\n",
    "TensorFlowを使いロジスティック回帰を実装していきます。入門1では単純な足し算でしたが、ここでは学習を伴う計算を行います。\n",
    "\n",
    "\n",
    "論理回路\n",
    "簡単な題材として、ロジスティック回帰による論理回路の再現を行います。論理回路は2つの値を入力し、1つの値を出力する関数のようなものです。入力も出力も0か1のみで表され、入力される組み合わせによって出力する値が変わります。\n",
    "\n",
    "\n",
    "ANDゲートは入力された2つの値が両方とも1だった場合、出力が1となり、それ以外の組み合わせでは0を出力します。\n",
    "\n",
    "データの作成\n",
    "最初に学習用のトレーニングデータをNumPyにより作成しておきます。ANDゲートでは入力が2次元で出力が1次元となります。2つの入力が1のときだけ1を出力し、それ以外は0を出力するので以下のように定義できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x_train = np.array([[0,0],[0,1],[1,0],[1,1]])#[1,1]の時に[1]を出力する\n",
    "y_train = np.array([[0],[0],[0],[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データフローグラフの構築\n",
    "まずはデータフローグラフを構築します。\n",
    "\n",
    "\n",
    "学習データをTensorFlowのデータフローグラフに入力するための placeholder を用意しましょう。placeholderはデータフローグラフを作成する段階では値が決まっていない、空箱のような存在でした。\n",
    "第一引数のtf.float32で行列要素の数値のデータ型を指定しています。第二引数の[None,2]で行列の形を指定しています。ここで定義されている2はデータの次元を表しています。Noneの部分はデータ数を表す部分です。今回のANDゲートの場合のデータ数は[0,0],[0,1],[1,0],[1,1]の4つしかないのでNoneの部分を[4,2]としても問題はありません。しかし、任意の数のデータを入れられるように、一般的にはNoneを使います。\n",
    "？よくわかりません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "x = tf.placeholder(tf.float32, [None, 2])\n",
    "t = tf.placeholder(tf.float32, [None, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重みとバイアスの Valiable を用意します。Valiableとして用意するということは、これらが学習により更新を行う値であることを示します。\n",
    "ここで、tf.Variable()の中でtf.zeros()という関数を呼び出していますが、初期値として0を入れているということです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([2,1]))\n",
    "b = tf.Variable(tf.zeros([1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次にモデルの出力y（＝仮定関数）と目的関数を定義します。\n",
    "ロジスティック回帰の式は以下でした。ここでは正則化項は抜かしています。\n",
    "$$\n",
    "h_θ(x) = g(θ^T x).\\\\<br/>g(z) = \\frac{1}{1+e^{−z}}.\\\\<br/>J(\\theta)=  \\frac{1}{m}  \\sum_{i=1}^{m}[−y^{(i)} log(h_θ(x^{(i)})) − (1−y^{(i)}) log(1−h_θ(x^{(i)}))]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これをTensorFlowで記述すると次のようになります。\n",
    "tf.matmul()はNumPyにおけるnp.dot()に相当するベクトルの内積や、行列積を計算するためのメソッドです\n",
    "なお、例えば回帰問題で二乗和誤差関数を使用するのであれば、tf.reduce_sum(tf.square(y - t))というように定義できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.sigmoid(tf.matmul(x, W) + b)\n",
    "cross_entropy = tf.reduce_sum(-t * tf.log(y) - (1 - t) * tf.log(1 - y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここまでで、入力のための空箱であるplaceholderと学習可能なValiableをメソッドで結ぶことができました。\n",
    "学習を行うために、勾配降下法を用いてパラメータを最適化するためのコードを加えます。目的関数をGradientDescentOptimizerに渡します。GradientDescentOptimizer()は引数で学習率を指定しています。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習後の結果の正解が正しいかどうかの判定と正解率の計算もデータフローグラフとして定義できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.sign(y - 0.5), tf.sign(t - 0.5))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1行目で結果が正解かどうか判定しています。一つ一つ見ていきましょう。まずtf.equal()は引数に指定された2つの値が等しいかどうかを判定してくれます。返り値はBool値です。tf.sign()は引数の値が正なら1、0なら0、負なら-1を返します。yが0.5以上かどうかで結果が決まるので、y-0.5とt-0.5の符号を比較しています。\n",
    "\n",
    "\n",
    "2行目は正解率を計算するためのコードです。tf.reduce_mean()は多次元配列の各成分の平均を計算する関数です。tf.cast()でBool値を0,1に変換しています。つまりここでは正解で1、不正解で0と判定された配列の平均値をとっているので正解率を表していることになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データを入力して計算\n",
    "セッションを準備してパラメータを最適化する計算を行います。\n",
    "\n",
    "まずセッションのインスタンスを作成します。そして、tf.global_variables_initializer()によって上で定義したtf.Variable()の値(重み・バイアス)を初期化します。実行する際にはsess.run()を使います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, Accuracy: 0.750000\n",
      "epoch: 100, Accuracy: 1.000000\n",
      "epoch: 200, Accuracy: 1.000000\n",
      "epoch: 300, Accuracy: 1.000000\n",
      "epoch: 400, Accuracy: 1.000000\n",
      "epoch: 500, Accuracy: 1.000000\n",
      "epoch: 600, Accuracy: 1.000000\n",
      "epoch: 700, Accuracy: 1.000000\n",
      "epoch: 800, Accuracy: 1.000000\n",
      "epoch: 900, Accuracy: 1.000000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    sess.run(train_step, feed_dict={x:x_train,t:y_train})\n",
    "# 100回ごとに正解率を表示\n",
    "    if epoch % 100 == 0:\n",
    "        acc_val = sess.run(\n",
    "            accuracy, feed_dict={\n",
    "                x:x_train,\n",
    "                t:y_train})\n",
    "        print ('epoch: %d, Accuracy: %f'\n",
    "               %(epoch, acc_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2行目、sess.run()に上で定義したtrain_stepを入れることで、勾配降下法による学習を行っています。\n",
    "8行目はsess.run()にaccuracyを入れることで、正解率を計算しています。計算結果がNumPy形式で返ってきているので、これをprintします。\n",
    "形だけ定義していたplaceholderのxとtの中には値が何も入っていません。placeholderに値を設定するためにsess.run()のパラメータでfeed_dictを指定します。例えば、feed_dict={x:x_train,t:y_train})と書くことで空箱だったxにx_trainの値が入り、tにy_trainの値が入ります。\n",
    "表示結果を見てみると、正解率が100%でうまく学習できているように見えます。\n",
    "\n",
    "最後に各サンプルの計算結果を確認してロジスティック回帰の実装を終わります。先ほどのaccuracyと同様、実行することで計算結果が返ってくるためprintします。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "[[1.9651403e-04]\n",
      " [4.9049813e-02]\n",
      " [4.9049813e-02]\n",
      " [9.3120378e-01]]\n"
     ]
    }
   ],
   "source": [
    "#学習結果が正しいか確認\n",
    "classified = sess.run(correct_prediction, feed_dict={\n",
    "    x:x_train,\n",
    "    t:y_train\n",
    "})\n",
    "#出力yの確認\n",
    "prob = sess.run(y, feed_dict={\n",
    "    x:x_train,\n",
    "    t:y_train\n",
    "})\n",
    "print(classified)\n",
    "# [[ True]\n",
    "# [ True]\n",
    "# [ True]\n",
    "# [ True]]\n",
    "print(prob)\n",
    "# [[  1.96514215e-04]\n",
    "# [  4.90498319e-02]\n",
    "# [  4.90498319e-02]\n",
    "# [  9.31203783e-01]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classifiedの結果は全てTrueで正しく学習されていることがわかります。probは上からほぼ[0,0,0,1]となっています。今回活性化関数に用いたのはシグモイド関数ですので出力yは確率として表示されています。上から3つは1になる確率がほぼ0％、一番下の1つは93％程度の確率で1になるということになります。\n",
    "\n",
    "\n",
    "Wとbが学習後どのような値になっているかも見ておきましょう。Variableもsess.run()に入れることで値を確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [[5.569955]\n",
      " [5.569955]]\n",
      "b: [-8.53458]\n"
     ]
    }
   ],
   "source": [
    "print('W:', sess.run(W))\n",
    "print('b:', sess.run(b))\n",
    "# W: [[ 5.5699544]\n",
    "# [ 5.5699544]]\n",
    "# b: [-8.53457928]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  【問題1】スクラッチを振り返る\n",
    "ここまでのスクラッチを振り返り、ディープラーニングを実装するためにはどのようなものが必要だったかを列挙してください。\n",
    "\n",
    "\n",
    "重みを初期化する必要があった\n",
    "エポックのループが必要だった\n",
    "\n",
    "損失関数を設定する必要があった  \n",
    "誤差逆伝播法によって重みパラメータの勾配の計算  \n",
    "\n",
    "\n",
    "それらがフレームワークにおいてはどのように実装されるかを今回覚えていきましょう。\n",
    "\n",
    "\n",
    "データセットの用意\n",
    "以前から使用しているIrisデータセットを使用します。以下のサンプルコードではIris.csvが同じ階層にある想定です。\n",
    "\n",
    "\n",
    "Iris Species\n",
    "\n",
    "\n",
    "目的変数はSpeciesですが、3種類ある中から以下の2種類のみを取り出して使用します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題2】スクラッチとTensorFlowの対応を考える\n",
    "以下のサンプルコードを見て、先ほど列挙した「ディープラーニングを実装するために必要なもの」がTensorFlowではどう実装されているかを確認してください。\n",
    "\n",
    "\n",
    "それを簡単に言葉でまとめてください。単純な一対一の対応であるとは限りません。\n",
    "\n",
    "・重みの初期値はtf.Variable(tf.random_normal([ , ]))である。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/apple/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/apple/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/apple/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/apple/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/apple/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/apple/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/apple/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/apple/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/apple/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/apple/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/apple/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/apple/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 0, loss : 1.8218, val_loss : 4.5825, acc : 0.750, val_acc : 0.688\n",
      "Epoch 1, loss : 0.0036, val_loss : 2.2782, acc : 1.000, val_acc : 0.875\n",
      "Epoch 2, loss : 0.0001, val_loss : 2.9416, acc : 1.000, val_acc : 0.750\n",
      "Epoch 3, loss : 0.0110, val_loss : 3.2522, acc : 1.000, val_acc : 0.812\n",
      "Epoch 4, loss : 0.0000, val_loss : 0.7246, acc : 1.000, val_acc : 0.812\n",
      "Epoch 5, loss : 0.0005, val_loss : 2.8904, acc : 1.000, val_acc : 0.875\n",
      "Epoch 6, loss : 0.0000, val_loss : 2.4210, acc : 1.000, val_acc : 0.812\n",
      "Epoch 7, loss : 0.5958, val_loss : 5.2623, acc : 0.750, val_acc : 0.750\n",
      "Epoch 8, loss : 0.0000, val_loss : 1.5183, acc : 1.000, val_acc : 0.812\n",
      "Epoch 9, loss : 0.0687, val_loss : 5.0111, acc : 1.000, val_acc : 0.750\n",
      "test_acc : 0.800\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TensorFlowで実装したニューラルネットワークを使いIrisデータセットを2値分類する\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "#iris_dataset= load_iris()\n",
    "# データセットの読み込み\n",
    "dataset_path =\"Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "# データフレームから条件抽出\n",
    "df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "# ラベルを数値に変換\n",
    "y[y=='Iris-versicolor'] = 0\n",
    "y[y=='Iris-virginica'] = 1\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "n_hidden1 = 50#重み1の列数、バイアス1の数、重み2の行数\n",
    "n_hidden2 = 100#重み2の列数、バイアス2の数、重み３の行数\n",
    "n_input = X_train.shape[1]#特徴量\n",
    "n_samples = X_train.shape[0]#サンプル数\n",
    "n_classes = 1#yは１列だから1、ラベルは分類の数だけある、予測値の列数(1)\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])#空箱、[データ数,次元数]\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "#NNの適当な名前\n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),#学習により更新を行う値\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    #tf.addは加算（matmalとb)\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    #tf.nn.reluで\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X)#outputをlogitsで受け取る\n",
    "\n",
    "# 目的関数、labels=ターゲット変数、logitsにoutputでlossを出す、tf.reduce_mean全ての要素を足し合わせて平均値を出力\n",
    "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))#logitsはlayer_output、yハット\n",
    "# 最適化手法、重みの更新の調整\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)#train_opはミニバッチのループ内で使う\n",
    "# 推定結果\n",
    "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n",
    "#↑データフローグラフの構築\n",
    "# 計算グラフの実行(データを入力して計算)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)#不明、訓練データのサンプル数をバッチサイズで割る\n",
    "        total_loss = 0#なんのため？\n",
    "        total_acc = 0#なんのため？\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= n_samples\n",
    "        total_acc /= n_samples\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "他のデータセットへの適用\n",
    "\n",
    "これまで扱ってきた小さなデータセットがいくつかあります。上記サンプルコードを書き換え、これらに対して学習・推定を行うニューラルネットワークを作成してください。\n",
    "\n",
    "\n",
    "Iris（3種類全ての目的変数を使用）\n",
    "House Prices\n",
    "\n",
    "どのデータセットもtrain, val, testの3種類に分けて使用してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題3】3種類全ての目的変数を使用したIrisのモデルを作成\n",
    "Irisデータセットのtrain.csvの中で、目的変数Speciesに含まれる3種類全てを分類できるモデルを作成してください。\n",
    "\n",
    "\n",
    "Iris Species\n",
    "\n",
    "\n",
    "2クラスの分類と3クラス以上の分類の違いを考慮してください。それがTensorFlowでどのように書き換えられるかを公式ドキュメントなどを参考に調べてください。\n",
    "\n",
    "\n",
    "《ヒント》\n",
    "\n",
    "\n",
    "以下の2箇所は2クラス分類特有の処理です。\n",
    "メソッドは以下のように公式ドキュメントを確認してください。\n",
    "\n",
    "ロジスティック回帰は２クラス分類\n",
    "Q.多クラス分類の処理を探す\n",
    "tf.nn.sigmoid_cross_entropy_with_logits  |  TensorFlow\n",
    "\n",
    "\n",
    "tf.math.sign  |  TensorFlow\n",
    "\n",
    "\n",
    "＊tf.signとtf.math.signは同じ働きをします。要素ごとに正なら1、0なら0、負なら-1となる変換をかける。\n",
    "何のため？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "\n",
    "シグモイド二値分類\n",
    "ソフトマックスに変えるのはなぜ\n",
    "\n",
    " tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
    " クロスエントロピーは多クラス分類でよく使われる誤差関数の一つで、dd番目のデータに対して、クラスiiである推定確率xd,ixd,iと真の確率yd,iyd,iの誤差を計算します。\n",
    " クロスエントロピーについてはTensorFlowでは単体の関数は用意されていないようなので、ソフトマックス関数とクロスエントロピーをセットにした、tf.nn.softmax_cross_entropy_with_logits_v2やtf.nn.sparse_softmax_cross_entropy_with_logitsを使います。\"with_logits\"というのは、この関数は内部でソフトマックスも計算するから、ニューラルネットワークの出力をそのまま入力してね、という意味です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path =\"Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "# データフレームから条件抽出\n",
    "\n",
    "#df = df[df]\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "# ラベルを数値に変換\n",
    "y[y=='Iris-setosa'] = 0\n",
    "y[y=='Iris-versicolor'] = 1\n",
    "y[y=='Iris-virginica'] = 2\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "\n",
    "#yのOnehot化\n",
    "enc = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)#インスタンス化\n",
    "y = enc.fit_transform(y)#fit_transform、データを関数に適合させるだけではなく、形も変形する\n",
    "#fitはデータに関数を適合させる、真の関数に近似する関数を作る\n",
    "\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 102.8792, val_loss : 31.0601, acc : 0.500, val_acc : 0.458\n",
      "Epoch 1, loss : 8.7579, val_loss : 8.4823, acc : 0.800, val_acc : 0.833\n",
      "Epoch 2, loss : 1.9219, val_loss : 2.7885, acc : 0.890, val_acc : 0.875\n",
      "Epoch 3, loss : 1.7209, val_loss : 1.5249, acc : 0.910, val_acc : 0.917\n",
      "Epoch 4, loss : 0.8113, val_loss : 3.4528, acc : 0.960, val_acc : 0.833\n",
      "Epoch 5, loss : 1.1159, val_loss : 0.8146, acc : 0.940, val_acc : 0.958\n",
      "Epoch 6, loss : 0.6522, val_loss : 0.0861, acc : 0.960, val_acc : 0.958\n",
      "Epoch 7, loss : 0.5404, val_loss : 1.8063, acc : 0.970, val_acc : 0.875\n",
      "Epoch 8, loss : 1.0847, val_loss : 0.5066, acc : 0.920, val_acc : 0.958\n",
      "Epoch 9, loss : 0.7439, val_loss : 0.9573, acc : 0.960, val_acc : 0.917\n",
      "Epoch 10, loss : 0.2575, val_loss : 0.4853, acc : 0.960, val_acc : 0.958\n",
      "Epoch 11, loss : 0.1783, val_loss : 0.2291, acc : 0.970, val_acc : 0.958\n",
      "Epoch 12, loss : 0.1876, val_loss : 0.3406, acc : 0.970, val_acc : 0.958\n",
      "Epoch 13, loss : 0.1811, val_loss : 0.3326, acc : 0.970, val_acc : 0.958\n",
      "Epoch 14, loss : 0.2154, val_loss : 0.3669, acc : 0.960, val_acc : 0.958\n",
      "Epoch 15, loss : 0.3100, val_loss : 0.8131, acc : 0.940, val_acc : 0.875\n",
      "Epoch 16, loss : 0.4245, val_loss : 1.5632, acc : 0.920, val_acc : 0.833\n",
      "Epoch 17, loss : 0.6201, val_loss : 1.7190, acc : 0.920, val_acc : 0.833\n",
      "Epoch 18, loss : 0.7863, val_loss : 0.6985, acc : 0.920, val_acc : 0.917\n",
      "Epoch 19, loss : 0.1476, val_loss : 0.3632, acc : 0.980, val_acc : 0.958\n",
      "Epoch 20, loss : 0.2689, val_loss : 0.0373, acc : 0.980, val_acc : 1.000\n",
      "Epoch 21, loss : 0.0968, val_loss : 0.0833, acc : 0.980, val_acc : 0.958\n",
      "Epoch 22, loss : 0.0987, val_loss : 0.1576, acc : 0.970, val_acc : 0.958\n",
      "Epoch 23, loss : 0.1377, val_loss : 0.0657, acc : 0.960, val_acc : 0.958\n",
      "Epoch 24, loss : 0.0855, val_loss : 0.0847, acc : 0.960, val_acc : 0.958\n",
      "Epoch 25, loss : 0.0970, val_loss : 0.0866, acc : 0.970, val_acc : 0.958\n",
      "Epoch 26, loss : 0.1122, val_loss : 0.0877, acc : 0.960, val_acc : 0.917\n",
      "Epoch 27, loss : 0.1075, val_loss : 0.0830, acc : 0.960, val_acc : 0.917\n",
      "Epoch 28, loss : 0.0952, val_loss : 0.0744, acc : 0.960, val_acc : 0.917\n",
      "Epoch 29, loss : 0.0951, val_loss : 0.0710, acc : 0.960, val_acc : 0.958\n",
      "Epoch 30, loss : 0.0918, val_loss : 0.0704, acc : 0.960, val_acc : 0.958\n",
      "Epoch 31, loss : 0.0862, val_loss : 0.0721, acc : 0.960, val_acc : 0.958\n",
      "Epoch 32, loss : 0.0856, val_loss : 0.0754, acc : 0.960, val_acc : 0.958\n",
      "Epoch 33, loss : 0.0854, val_loss : 0.0774, acc : 0.960, val_acc : 0.958\n",
      "Epoch 34, loss : 0.0821, val_loss : 0.0763, acc : 0.960, val_acc : 0.958\n",
      "Epoch 35, loss : 0.0824, val_loss : 0.0805, acc : 0.960, val_acc : 0.958\n",
      "Epoch 36, loss : 0.0798, val_loss : 0.0537, acc : 0.960, val_acc : 0.958\n",
      "Epoch 37, loss : 0.0931, val_loss : 0.0782, acc : 0.970, val_acc : 0.958\n",
      "Epoch 38, loss : 0.0888, val_loss : 0.0770, acc : 0.970, val_acc : 0.917\n",
      "Epoch 39, loss : 0.0669, val_loss : 0.0600, acc : 0.960, val_acc : 0.958\n",
      "Epoch 40, loss : 0.2329, val_loss : 0.5667, acc : 0.953, val_acc : 0.792\n",
      "Epoch 41, loss : 0.2835, val_loss : 0.0676, acc : 0.930, val_acc : 0.958\n",
      "Epoch 42, loss : 0.2279, val_loss : 0.0723, acc : 0.940, val_acc : 0.958\n",
      "Epoch 43, loss : 0.0853, val_loss : 0.0824, acc : 0.970, val_acc : 0.958\n",
      "Epoch 44, loss : 0.2226, val_loss : 0.0558, acc : 0.970, val_acc : 1.000\n",
      "Epoch 45, loss : 0.2244, val_loss : 0.5691, acc : 0.960, val_acc : 0.917\n",
      "Epoch 46, loss : 0.0482, val_loss : 0.0526, acc : 0.980, val_acc : 1.000\n",
      "Epoch 47, loss : 0.1180, val_loss : 0.1812, acc : 0.970, val_acc : 0.917\n",
      "Epoch 48, loss : 0.0576, val_loss : 0.0509, acc : 0.990, val_acc : 0.958\n",
      "Epoch 49, loss : 0.0806, val_loss : 0.1109, acc : 0.970, val_acc : 0.917\n",
      "Epoch 50, loss : 0.0631, val_loss : 0.0618, acc : 0.980, val_acc : 0.958\n",
      "Epoch 51, loss : 0.0770, val_loss : 0.0972, acc : 0.980, val_acc : 0.958\n",
      "Epoch 52, loss : 0.0668, val_loss : 0.0665, acc : 0.980, val_acc : 0.958\n",
      "Epoch 53, loss : 0.0711, val_loss : 0.0717, acc : 0.980, val_acc : 0.958\n",
      "Epoch 54, loss : 0.0720, val_loss : 0.0535, acc : 0.980, val_acc : 0.958\n",
      "Epoch 55, loss : 0.0791, val_loss : 0.0917, acc : 0.970, val_acc : 0.958\n",
      "Epoch 56, loss : 0.0663, val_loss : 0.0594, acc : 0.990, val_acc : 0.958\n",
      "Epoch 57, loss : 0.0719, val_loss : 0.0651, acc : 0.970, val_acc : 0.958\n",
      "Epoch 58, loss : 0.0645, val_loss : 0.0555, acc : 0.990, val_acc : 0.958\n",
      "Epoch 59, loss : 0.0639, val_loss : 0.0609, acc : 0.980, val_acc : 0.958\n",
      "Epoch 60, loss : 0.0703, val_loss : 0.0751, acc : 0.970, val_acc : 0.958\n",
      "Epoch 61, loss : 0.0683, val_loss : 0.0654, acc : 0.980, val_acc : 0.958\n",
      "Epoch 62, loss : 0.0673, val_loss : 0.0657, acc : 0.980, val_acc : 0.958\n",
      "Epoch 63, loss : 0.0628, val_loss : 0.0606, acc : 0.990, val_acc : 0.958\n",
      "Epoch 64, loss : 0.0668, val_loss : 0.0606, acc : 0.980, val_acc : 0.958\n",
      "Epoch 65, loss : 0.0651, val_loss : 0.0563, acc : 0.970, val_acc : 0.958\n",
      "Epoch 66, loss : 0.0646, val_loss : 0.0635, acc : 0.990, val_acc : 0.958\n",
      "Epoch 67, loss : 0.0614, val_loss : 0.0582, acc : 0.990, val_acc : 0.958\n",
      "Epoch 68, loss : 0.0602, val_loss : 0.0549, acc : 0.980, val_acc : 0.958\n",
      "Epoch 69, loss : 0.0664, val_loss : 0.0728, acc : 0.970, val_acc : 0.958\n",
      "Epoch 70, loss : 0.0612, val_loss : 0.0564, acc : 0.980, val_acc : 0.958\n",
      "Epoch 71, loss : 0.0583, val_loss : 0.0656, acc : 0.990, val_acc : 0.958\n",
      "Epoch 72, loss : 0.0628, val_loss : 0.0822, acc : 0.990, val_acc : 0.958\n",
      "Epoch 73, loss : 0.0617, val_loss : 0.0667, acc : 0.990, val_acc : 0.958\n",
      "Epoch 74, loss : 0.0619, val_loss : 0.0586, acc : 0.990, val_acc : 0.958\n",
      "Epoch 75, loss : 0.0536, val_loss : 0.0607, acc : 0.990, val_acc : 0.958\n",
      "Epoch 76, loss : 0.0540, val_loss : 0.0582, acc : 0.990, val_acc : 0.958\n",
      "Epoch 77, loss : 0.0671, val_loss : 0.0796, acc : 0.980, val_acc : 0.958\n",
      "Epoch 78, loss : 0.0600, val_loss : 0.0428, acc : 0.980, val_acc : 1.000\n",
      "Epoch 79, loss : 0.0523, val_loss : 0.0520, acc : 0.990, val_acc : 0.958\n",
      "Epoch 80, loss : 0.0489, val_loss : 0.0706, acc : 0.990, val_acc : 0.958\n",
      "Epoch 81, loss : 0.0674, val_loss : 0.0796, acc : 0.970, val_acc : 0.958\n",
      "Epoch 82, loss : 0.1399, val_loss : 0.1463, acc : 0.940, val_acc : 0.958\n",
      "Epoch 83, loss : 0.2501, val_loss : 0.3408, acc : 0.960, val_acc : 0.917\n",
      "Epoch 84, loss : 0.2483, val_loss : 0.4338, acc : 0.970, val_acc : 0.917\n",
      "Epoch 85, loss : 0.1899, val_loss : 0.0391, acc : 0.960, val_acc : 0.958\n",
      "Epoch 86, loss : 0.0834, val_loss : 0.0441, acc : 0.960, val_acc : 0.958\n",
      "Epoch 87, loss : 0.0809, val_loss : 0.1347, acc : 0.960, val_acc : 0.917\n",
      "Epoch 88, loss : 0.1360, val_loss : 0.3079, acc : 0.940, val_acc : 0.958\n",
      "Epoch 89, loss : 0.0835, val_loss : 0.2108, acc : 0.960, val_acc : 0.958\n",
      "Epoch 90, loss : 0.0659, val_loss : 0.0830, acc : 0.970, val_acc : 0.958\n",
      "Epoch 91, loss : 0.1162, val_loss : 0.2476, acc : 0.950, val_acc : 0.917\n",
      "Epoch 92, loss : 0.5372, val_loss : 6.4620, acc : 0.903, val_acc : 0.792\n",
      "Epoch 93, loss : 0.5097, val_loss : 1.6496, acc : 0.927, val_acc : 0.917\n",
      "Epoch 94, loss : 3.9940, val_loss : 4.4301, acc : 0.753, val_acc : 0.667\n",
      "Epoch 95, loss : 1.0528, val_loss : 1.9087, acc : 0.847, val_acc : 0.917\n",
      "Epoch 96, loss : 0.5172, val_loss : 0.9087, acc : 0.903, val_acc : 0.917\n",
      "Epoch 97, loss : 0.4657, val_loss : 0.2458, acc : 0.910, val_acc : 0.917\n",
      "Epoch 98, loss : 0.1525, val_loss : 0.2828, acc : 0.930, val_acc : 0.917\n",
      "Epoch 99, loss : 0.5928, val_loss : 1.2078, acc : 0.803, val_acc : 0.792\n",
      "test_acc : 0.800\n"
     ]
    }
   ],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):#目的？\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.1\n",
    "batch_size = 10\n",
    "num_epochs = 100\n",
    "n_hidden1 = 50#重み1の列数、バイアス1の数、重み2の行数\n",
    "n_hidden2 = 100#重み2の列数、バイアス2の数、重み３の行数\n",
    "n_input = X_train.shape[1]#特徴量\n",
    "n_samples = X_train.shape[0]#サンプル数\n",
    "n_classes = 3#yは１列だから1、ラベルは分類の数だけある、予測値の列数(1)\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])#空箱、[データ数,次元数]\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "#NNの適当な名前\n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),#学習により更新を行う値\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    #tf.addは加算（matmalとb)\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    #tf.nn.reluで\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X)#outputをlogitsで受け取る\n",
    "\n",
    "# 目的関数、labels=ターゲット変数、logitsにoutputでlossを出す、tf.reduce_mean全ての要素を足し合わせて平均値を出力\n",
    "#loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))#logitsはlayer_output、yハット\n",
    "loss_op = tf.reduce_mean( tf.losses.softmax_cross_entropy(onehot_labels=Y, logits=logits)) \n",
    "#with_logits\"というのは、この関数は内部でソフトマックスも計算するから、ニューラルネットワークの出力をそのまま入力\n",
    "# 最適化手法、重みの更新の調整\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)#train_opはミニバッチのループ内で使う\n",
    "# 推定結果\n",
    "#correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))#(tf.sigmoid(logits) - 0.5))\n",
    "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1)) #遠藤さん、何をしている？何番目のアイリスかを比較している\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n",
    "#↑データフローグラフの構築\n",
    "# 計算グラフの実行(データを入力して計算)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)#不明、訓練データのサンプル数をバッチサイズで割る\n",
    "        total_loss = 0#なんのため？\n",
    "        total_acc = 0#なんのため？\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):#ミニバッチ\n",
    "            # ミニバッチごとにループ\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= total_batch\n",
    "        total_acc /= total_batch\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, total_loss, val_loss, total_acc, val_acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "あ [[ True  True  True]\n",
      " [False False  True]\n",
      " [False False  True]\n",
      " [ True  True  True]\n",
      " [False False  True]\n",
      " [ True False False]\n",
      " [ True False False]\n",
      " [ True False False]\n",
      " [ True False False]\n",
      " [False False  True]\n",
      " [False False  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [False False  True]\n",
      " [ True False False]\n",
      " [ True False False]\n",
      " [ True  True  True]\n",
      " [False False  True]\n",
      " [ True False False]\n",
      " [ True False False]\n",
      " [ True  True  True]\n",
      " [False False  True]\n",
      " [False False  True]\n",
      " [ True False False]]\n",
      "あ [[False  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [False  True  True]\n",
      " [ True False  True]\n",
      " [False False False]\n",
      " [False False False]\n",
      " [False False False]\n",
      " [False False False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [False  True  True]\n",
      " [False  True  True]\n",
      " [ True False  True]\n",
      " [False False False]\n",
      " [False False False]\n",
      " [False  True  True]\n",
      " [ True False  True]\n",
      " [False False False]\n",
      " [False False False]\n",
      " [False  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [False False False]]\n",
      "あ [[False False False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [False False False]\n",
      " [ True  True False]\n",
      " [False  True  True]\n",
      " [False  True  True]\n",
      " [False  True  True]\n",
      " [False  True  True]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [False False False]\n",
      " [False False False]\n",
      " [ True  True False]\n",
      " [False  True  True]\n",
      " [False  True  True]\n",
      " [False False False]\n",
      " [ True  True False]\n",
      " [False  True  True]\n",
      " [False  True  True]\n",
      " [False False False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [False  True  True]]\n",
      "あ [[False False False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [False False False]\n",
      " [ True  True False]\n",
      " [False  True  True]\n",
      " [False  True  True]\n",
      " [False  True  True]\n",
      " [False  True  True]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [False False False]\n",
      " [False False False]\n",
      " [ True  True False]\n",
      " [False  True  True]\n",
      " [False  True  True]\n",
      " [False False False]\n",
      " [ True  True False]\n",
      " [False  True  True]\n",
      " [False  True  True]\n",
      " [False False False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [False  True  True]]\n",
      "あ [[ True False False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True False False]\n",
      " [ True  True False]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True False False]\n",
      " [ True False False]\n",
      " [ True  True False]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True False False]\n",
      " [ True  True False]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True False False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True  True]]\n",
      "あ [[ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True False False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True  True]]\n",
      "あ [[ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True False  True]]\n",
      "あ [[ True  True False]\n",
      " [ True False False]\n",
      " [ True False False]\n",
      " [ True  True False]\n",
      " [ True False False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False False]\n",
      " [ True False False]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True False False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False False]\n",
      " [ True False False]\n",
      " [ True False  True]]\n",
      "あ [[ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]]\n",
      "あ [[ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]]\n",
      "Epoch 0, total_loss : 97.1705, val_loss : 93.8317, total_acc : 0.580, val_acc : 0.592\n",
      "あ [[ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]]\n",
      "あ [[ True  True False]\n",
      " [ True False  True]\n",
      " [ True  True  True]\n",
      " [ True  True False]\n",
      " [ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True  True]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True False  True]]\n",
      "あ [[ True  True False]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True False]\n",
      " [ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True False  True]]\n",
      "あ [[ True  True False]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True False]\n",
      " [ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True False  True]]\n",
      "あ [[ True  True False]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True False]\n",
      " [ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [False False False]\n",
      " [ True  True False]\n",
      " [ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [False False False]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True False  True]]\n",
      "あ [[False  True False]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True False]\n",
      " [ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [False False False]\n",
      " [False  True False]\n",
      " [ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [False False False]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True False  True]]\n",
      "あ [[False  True False]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True False]\n",
      " [ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [False  True False]\n",
      " [False  True False]\n",
      " [ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [False  True False]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True False  True]]\n",
      "あ [[False  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [False  True  True]\n",
      " [False  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [False  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]]\n",
      "あ [[ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]]\n",
      "あ [[ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]]\n",
      "Epoch 1, total_loss : 14.0579, val_loss : 12.9260, total_acc : 0.699, val_acc : 0.729\n",
      "あ [[ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]]\n",
      "あ [[ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]]\n",
      "あ [[ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]]\n",
      "あ [[ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]]\n",
      "あ [[ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]]\n",
      "あ [[ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]]\n",
      "あ [[ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]]\n",
      "あ [[ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]]\n",
      "あ [[ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]]\n",
      "あ [[ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]]\n",
      "Epoch 2, total_loss : 3.8975, val_loss : 3.2201, total_acc : 0.683, val_acc : 0.688\n",
      "あ [[ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]]\n",
      "あ [[ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]]\n",
      "あ [[ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]]\n",
      "あ [[ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]]\n",
      "あ [[ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]]\n",
      "あ [[ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]]\n",
      "あ [[ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]]\n",
      "あ [[ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]]\n",
      "あ [[ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]]\n",
      "あ [[ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]]\n",
      "Epoch 3, total_loss : 2.3245, val_loss : 1.3411, total_acc : 0.670, val_acc : 0.675\n",
      "あ [[ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]]\n",
      "あ [[ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]]\n",
      "あ [[ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]]\n",
      "あ [[ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]]\n",
      "あ [[ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]]\n",
      "あ [[ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]]\n",
      "あ [[ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]]\n",
      "あ [[ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]]\n",
      "あ [[ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]]\n",
      "あ [[ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True  True False]\n",
      " [ True False  True]\n",
      " [ True False  True]\n",
      " [ True False  True]]\n",
      "Epoch 4, total_loss : 1.5347, val_loss : 1.3378, total_acc : 0.680, val_acc : 0.685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc : 0.678\n"
     ]
    }
   ],
   "source": [
    "#遠藤さん\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.1\n",
    "batch_size = 10\n",
    "num_epochs = 5\n",
    "n_hidden1 = 50#重み1の列数、バイアス1の数、重み2の行数\n",
    "n_hidden2 = 100#重み2の列数、バイアス2の数、重み３の行数\n",
    "n_input = X_train.shape[1]#特徴量\n",
    "n_samples = X_train.shape[0]#サンプル数\n",
    "n_classes = 3#yは１列だから1、ラベルは分類の数だけある、予測値の列数(1)\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])#空箱、[データ数,次元数]\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "#NNの適当な名前\n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),#学習により更新を行う値\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    #tf.addは加算（matmalとb)\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    #tf.nn.reluで\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X)\n",
    "# 目的関数(3値分類)\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
    "# 最適化手法\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# 推定結果\n",
    "#correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1)) #遠藤さん、tf.argmax()： 第2パラメーターに1をセット、行ごとに最大となる列を返す\n",
    "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5)) #元々\n",
    "#correct_pred = tf.equal(tf.math.argmax(Y), tf.math.argmax(logits))\n",
    "# correct_pred = tf.argmax(tf.nn.softmax(logits), axis=1)\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))#同じ\n",
    "\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)#トータルバッチ\n",
    "        loss_list = []\n",
    "        acc_list = []\n",
    "        val_loss_list = []\n",
    "        acc_val_list = []\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})#不明\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            val_loss, val_acc, pred = sess.run([loss_op, accuracy,correct_pred], feed_dict={X: X_val, Y: y_val})\n",
    "            loss_list.append(loss)\n",
    "            acc_list.append(acc)\n",
    "            val_loss_list.append(val_loss)\n",
    "            acc_val_list.append(val_acc)\n",
    "            print('あ',pred)\n",
    "        print(\"Epoch {}, total_loss : {:.4f}, val_loss : {:.4f}, total_acc : {:.3f}, val_acc : {:.3f}\".format(\n",
    "            epoch, np.mean(loss_list), np.mean(val_loss_list), np.mean(acc_list), np.mean(acc_val_list)))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題4】House Pricesのモデルを作成\n",
    "回帰問題のデータセットであるHouse Pricesを使用したモデルを作成してください。\n",
    "\n",
    "\n",
    "House Prices: Advanced Regression Techniques\n",
    "\n",
    "\n",
    "この中のtrain.csvをダウンロードし、目的変数としてSalePrice、説明変数として、GrLivAreaとYearBuiltを使ってください。説明変数はさらに増やしても構いません。\n",
    "\n",
    "\n",
    "分類問題と回帰問題の違いを考慮してください。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 347.0931, val_loss : 139.8113, acc : 1.000, val_acc : 1.000\n",
      "Epoch 1, loss : 25.7954, val_loss : 0.8124, acc : 0.973, val_acc : 1.000\n",
      "Epoch 2, loss : 0.7703, val_loss : 0.5565, acc : 1.000, val_acc : 1.000\n",
      "Epoch 3, loss : 0.5875, val_loss : 0.7181, acc : 1.000, val_acc : 1.000\n",
      "Epoch 4, loss : 0.4800, val_loss : 0.6520, acc : 1.000, val_acc : 1.000\n",
      "Epoch 5, loss : 0.4858, val_loss : 0.4358, acc : 1.000, val_acc : 1.000\n",
      "Epoch 6, loss : 0.4623, val_loss : 0.4130, acc : 1.000, val_acc : 1.000\n",
      "Epoch 7, loss : 0.4974, val_loss : 0.3993, acc : 1.000, val_acc : 1.000\n",
      "Epoch 8, loss : 0.4567, val_loss : 0.4197, acc : 1.000, val_acc : 1.000\n",
      "Epoch 9, loss : 0.4779, val_loss : 0.4166, acc : 1.000, val_acc : 1.000\n",
      "test_acc : 1.000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TensorFlowで実装したニューラルネットワークを使いIrisデータセットを2値分類する\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# データセットの読み込み\n",
    "dataset_path =\"train.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "# データフレームから条件抽出\n",
    "y = df[\"SalePrice\"]\n",
    "X = df.loc[:  , [\"GrLivArea\" , \"YearBuilt\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "# 正則化\n",
    "# ss = StandardScaler()\n",
    "# ss.fit(X)\n",
    "# X = ss.transform(X)\n",
    "#対数変換\n",
    "y = np.log(y)\n",
    "X = np.log(X)\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      学習データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 1e-3\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "\n",
    "n_hidden1 = 10\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 1\n",
    "\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output\n",
    "\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X)\n",
    "\n",
    "# 目的関数\n",
    "# 損失関数を指定(\n",
    "# L2損失関数を指定(RMSE)\n",
    "#loss = tf.sqrt(tf.reduce_mean(tf.square(y_target - model_output)))\n",
    "loss_op = tf.sqrt(tf.reduce_mean(tf.square(Y - logits)))\n",
    "# loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "# loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "# 最適化手法\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# 推定結果\n",
    "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
    "# correct_pred = tf.equal(tf.argmax(Y ,1), tf.argmax(logits , 1))\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)#不明、訓練データのサンプル数をバッチサイズで割る\n",
    "        total_loss = 0#なんのため？\n",
    "        total_acc = 0#なんのため？\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= total_batch\n",
    "        total_acc /= total_batch\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, total_loss, val_loss, total_acc, val_acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(init)\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # エポックごとにループ\n",
    "#         total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "#         total_loss = 0\n",
    "#         total_acc = 0\n",
    "#         for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "#             # ミニバッチごとにループ\n",
    "#             sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "#             loss = sess.run(loss_op , feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "#             total_loss += loss\n",
    "#             total_acc += acc\n",
    "#         total_loss /= n_samples\n",
    "#         total_acc /= n_samples\n",
    "#         val_loss = sess.run(loss_op, feed_dict={X: X_val, Y: y_val})\n",
    "#         print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}\".format(epoch, loss, val_loss))\n",
    "#         test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "#         print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題5】MNISTのモデルを作成\n",
    "ニューラルネットワークのスクラッチで使用したMNISTを分類するモデルを作成してください。\n",
    "\n",
    "\n",
    "3クラス以上の分類という点ではひとつ前のIrisと同様です。入力が画像であるという点で異なります。\n",
    "\n",
    "\n",
    "スクラッチで実装したモデルの再現を目指してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 0.6114, val_loss : 1.1027, acc : 0.800, val_acc : 0.717\n",
      "Epoch 1, loss : 0.4930, val_loss : 0.7755, acc : 0.800, val_acc : 0.824\n",
      "Epoch 2, loss : 0.2462, val_loss : 0.4959, acc : 0.900, val_acc : 0.894\n",
      "Epoch 3, loss : 0.1886, val_loss : 0.3368, acc : 0.900, val_acc : 0.916\n",
      "Epoch 4, loss : 0.1018, val_loss : 0.3349, acc : 0.900, val_acc : 0.925\n",
      "Epoch 5, loss : 0.0140, val_loss : 0.3058, acc : 1.000, val_acc : 0.930\n",
      "Epoch 6, loss : 0.0331, val_loss : 0.3364, acc : 1.000, val_acc : 0.932\n",
      "Epoch 7, loss : 0.0023, val_loss : 0.3067, acc : 1.000, val_acc : 0.931\n",
      "Epoch 8, loss : 0.0110, val_loss : 0.2885, acc : 1.000, val_acc : 0.931\n",
      "Epoch 9, loss : 0.0500, val_loss : 0.3229, acc : 1.000, val_acc : 0.931\n",
      "test_acc : 0.926\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TensorFlowで実装したニューラルネットワークを使いIrisデータセットを2値分類する\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "# データセットの読み込み\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(-1 , 784)\n",
    "X_test = X_test.reshape(-1 , 784)\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "# y_train = tf.one_hot(y_train , depth=3 ,dtype=None)\n",
    "# y_test = tf.one_hot(y_test , depth=3 , dtype=None)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test = enc.transform(y_test[:, np.newaxis])\n",
    "\n",
    "\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      学習データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 10\n",
    "\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output\n",
    "\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X)\n",
    "\n",
    "# 目的関数\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "# 最適化手法\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# 推定結果\n",
    "correct_pred = tf.equal(tf.argmax(Y ,1), tf.argmax(logits , 1))\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= n_samples\n",
    "        total_acc /= n_samples\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
