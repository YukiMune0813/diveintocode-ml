{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprint 論文読解\n",
    "以下の論文を読み問題に答えてください。CNNを使った物体検出（Object Detection）の代表的な研究です。\n",
    "\n",
    "[8]Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detection with region proposal networks. In: Advances in neural information processing systems. (2015) 91–99\n",
    "\n",
    "https://arxiv.org/pdf/1506.01497.pdf\n",
    "\n",
    "問題\n",
    "それぞれについてJupyter Notebookにマークダウン形式で記述してください。\n",
    "\n",
    "(1) 物体検出の分野にはどういった手法が存在したか。\n",
    "\n",
    "(2) Fasterとあるが、どういった仕組みで高速化したのか。\n",
    "\n",
    "(3) One-Stageの手法とTwo-Stageの手法はどう違うのか。\n",
    "\n",
    "(4) RPNとは何か。\n",
    "\n",
    "(5) RoIプーリングとは何か。\n",
    "\n",
    "(6) Anchorのサイズはどうするのが適切か。\n",
    "\n",
    "(7) 何というデータセットを使い、先行研究に比べどういった指標値が得られているか。\n",
    "\n",
    "(8) （アドバンス課題）Faster R-CNNよりも新しい物体検出の論文では、Faster R-CNNがどう引用されているか。\n",
    "\n",
    "条件\n",
    "答える際は論文のどの部分からそれが分かるかを書く。\n",
    "\n",
    "必要に応じて先行研究（引用されている論文）も探しにいく。最低2つは他の論文を利用して回答すること。\n",
    "\n",
    "論文の紹介記事を見ても良い。ただし、答えは論文内に根拠を探すこと。\n",
    "\n",
    "COCO   \n",
    "\n",
    "http://mscoco.org/\n",
    "\n",
    "以下の８０種類のカテゴリーが設定されており、コンペやトレーニングのモデルとしてよく活用されている\n",
    "\n",
    "‘person‘, ‘bicycle‘, ‘car‘, ‘motorcycle‘, ‘airplane‘, ‘bus‘, ‘train‘, ‘truck’, ‘boat‘, ‘traffic light’, ‘fire hydrant’, ‘stop sign’, ‘parking meter’, ‘bench’, ‘bird‘, ‘cat‘, ‘dog‘, ‘horse‘, ‘sheep‘, ‘cow‘, ‘elephant’, ‘bear’, ‘zebra’, ‘giraffe’, ‘backpack’, ‘umbrella’, ‘handbag’, ‘tie’, ‘suitcase’, ‘frisbee’, ‘skis’, ‘snowboard’, ‘sports ball’, ‘kite’, ‘baseball bat’, ‘baseball glove’, ‘skateboard’, ‘surfboard’, ‘tennis racket’, ‘bottle‘, ‘wine glass’, ‘cup’, ‘fork’, ‘knife’, ‘spoon’, ‘bowl’, ‘banana’, ‘apple’, ‘sandwich’, ‘orange’, ‘broccoli’, ‘carrot’, ‘hot dog’, ‘pizza’, ‘donut’, ‘cake’, ‘chair‘, ‘couch‘, ‘potted plant‘, ‘bed’, ‘dining table‘, ‘toilet’, ‘tv‘, ‘laptop’, ‘mouse’, ‘remote’, ‘keyboard’, ‘cell phone’, ‘microwave’, ‘oven’, ‘toaster’, ‘sink’, ‘refrigerator’, ‘book’, ‘clock’, ‘vase’, ‘scissors’, ‘teddy bear’, ‘hair drier’, ‘toothbrush’\n",
    "\n",
    "R-CNN\n",
    "物体検出のタスクに対してもCNNのアルゴリズムを応用できないかと課題定義された。（CNNは非常に遅く、計算上非常に高価で、スライディングウィンドウ検出器によって生成された非常に多くのパッチでCNNを実行することは不可能だった） R-CNNではSelective Search（複数のスケールのウィンドウを調べて、 テクスチャ、色、または強度を共有する隣接ピクセルを探し 物体を識別します。）と呼ばれる物体候補アルゴリズムを使用し、この問題を改善\n",
    "\n",
    "Fast-R-CNN\n",
    "R-CNNはネットワークの順伝播計算を提案された物体領域ごとに行う必要があったため、検出速度が遅いという問題点があった。 そこで１回計算した特徴マップを全ての提案された物体領域で再利用することで、R-CNNを高速化させたのがFast-R-CNN\n",
    "\n",
    "主に三つの層に分けられる。[Backbone] , [RPN] , [Head　：出力層] cis layerから「物体とみなすか否か」、reg layerから「オフセット」が得られる このcis layer ,reg layerそれぞれ損失を求め、それらを合算した損失をRPNの損失とする\n",
    "\n",
    "https://www.slideshare.net/windmdk/mask-rcnn https://qiita.com/shtmr/items/4283c851bc3d9721ed96\n",
    "\n",
    "non-maximum suppression\n",
    "non-maximum suppressionはIoU（領域の重なり具合を数値化したもの　IoU = (A^B) / (A∨B）)を利用して、同じクラスとして認識された重なっている状態の領域を抑制するためのアルゴリズムです IoU値の閾値を0.3という具合に定め、この閾値よりも大きいものを重複した物体領域の候補として外し、閾値に満たないものは、物体領域の候補として残すことになります。\n",
    "\n",
    "物体検出の結果として、検出対象物体を中心として複数のバウンディングボックスが検出されてしまうことがあるので、同一物体に複数のバウンディングボックスが検出されないようにするために、バウンディングボックスごとに検出の信頼度を表すスコアを計算し、局所的に最大スコアのバウンディングボックスのみを表示し、その他を表示しないように抑制する。その処理を 非最大値の抑制（non-maximum suppression　：　NMS） と呼ぶ\n",
    "\n",
    "FASTER R-CNN\n",
    "Our object detection system, called Faster R-CNN, is composed of two modules. The first module is a deep fully convolutional network that proposes regions, and the second module is the Fast R-CNN detector [2] that uses the proposed regions. The entire system is a single, unified network for object detection (Figure 2). Using the recently popular terminology of neural networks with ‘attention’ [31] mechanisms, the RPN module tells the Fast R-CNN module where to look. In Section 3.1 we introduce the designs and properties of the network for region proposal. In Section 3.2 we develop algorithms for training both modules with features shared.\n",
    "\n",
    "Faster R-CNNと呼ばれるオブジェクト検出システムは、2つのモジュールで構成されています。最初のモジュールは、領域を提案する完全な畳み込みネットワークであり、2番目のモジュールは、提案された領域を使用する高速R-CNN検出器です[ 2 ]。システム全体は、オブジェクト検出用の単一の統合ネットワークです（図 2）。「注意」[ 31 ]メカニズムを備えたニューラルネットワークの最近人気のある用語を使用して、RPNモジュールはFast R-CNNモジュールにどこを見ればよいかを伝えます。セクション 3.1では、地域提案のためのネットワークの設計と特性を紹介します。セクション3.2では 、機能を共有して両方のモジュールをトレーニングするアルゴリズムを開発します。\n",
    "\n",
    "https://medium.com/@jonathan_hui/image-segmentation-with-mask-r-cnn-ebe6d793272https://www.youtube.com/watch?v=XGi-Mz3do2s\n",
    "\n",
    "IoU\n",
    "IoUとは重複する面積の割合を評価し、IoUが閾値0.7以上であれば物体、IoUが閾値0.3以下であれば非物体、それ以外のアンカーは評価対象外となる\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 物体検出の分野にはどういった手法が存在したか。\n",
    "\n",
    "\n",
    "・Selective Search\n",
    "・CPMC\n",
    "・MCG\n",
    "・Fast R-CNN\n",
    "・OverFeat\n",
    "\n",
    "Object Proposals. There is a large literature on object proposal methods. Comprehensive surveys and comparisons of object proposal methods can be found in [19], [20], [21]. Widely used object proposal methods include those based on grouping super-pixels (e.g., Selective Search [4], CPMC [22], MCG [23]) and those based on sliding windows (e.g., objectness in windows [24], EdgeBoxes [6]). Object proposal methods were adopted as external modules independent of the detectors (e.g., Selective Search [4] object detectors, RCNN [5], and Fast R-CNN [2]).\n",
    "Several papers have proposed ways ofusing deep networks for predicting object boundingboxes [25], [9], [26], [27]. In the OverFeat method [9],a fully-connected layer is trained to predict the box coordinates for the localization task that assumes a\n",
    "single object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Fasterとあるが、どういった仕組みで高速化したのか。\n",
    "\n",
    "CNNから得られた特徴マップをインプットとしたRegion Proposal用のニューラルネットワーク (Region Proposal Network: RPN) を学習によって獲得することで、時間のかかるSelective Searchよりも高い推論速度を実現できるようになった。なお、Region Proposal以外はFast R-CNNを踏襲している。\n",
    "\n",
    "Faster R-CNNと呼ばれるオブジェクト検出システムは、2つのモジュールで構成されています。最初のモジュールは、領域を提案する完全な畳み込みネットワークであり、2番目のモジュールは、提案された領域を使用する高速R-CNN検出器です[ 2 ]。システム全体は、オブジェクト検出用の単一の統合ネットワークです（図 2）。「注意」[ 31 ]メカニズムを備えたニューラルネットワークの最近人気のある用語を使用して、RPNモジュールはFast R-CNNモジュールにどこを見ればよいかを伝えます。セクション 3.1では、地域提案のためのネットワークの設計と特性を紹介します。セクション3.2では 、機能を共有して両方のモジュールをトレーニングするアルゴリズムを開発します。\n",
    "\n",
    "\n",
    "Faster R-CNNと呼ばれるオブジェクト検出システムは２つのモジュールで構成されており、最初のモジュールは、RPN（領域を提案する畳み込みネットワーク)二番目のモジュールは、RPNを使用するFaster R-CNN\n",
    "Fast R-CNNまでは物体領域抽出にSelective Searchと呼ばれるディープラーニング以前の手法をしようしていたため、抽出性能が低く候補が無数に抽出され、物体候補領域の抽出とその後の識別処理の計算コストが膨大となってしまっていた\n",
    "\n",
    "Faster R-CNNでは、物体候補領域の抽出をディープラーニングモデルに取り込み、Endo-to-Endoで学習、推定することにより高性能な物体候補領域の抽出を実現し、その結果高速かつ高性能な一般物体検出を実現した\n",
    "\n",
    "Our object detection system, called Faster R-CNN, is composed of two modules. The first module is a deep fully convolutional network that proposes regions, and the second module is the Fast R-CNN detector [2] that uses the proposed regions. The entire system is a single, unified network for object detection (Figure 2). Using the recently popular terminology of neural networks with ‘attention’ [31] mechanisms, the RPN module tells the Fast R-CNN module where to look. In Section 3.1 we introduce the designs and properties of the network for region proposal. In Section 3.2 we develop algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) One-Stageの手法とTwo-Stageの手法はどう違うのか。\n",
    "\n",
    "\n",
    "One-Stageの手法とTwo-Stageの手法を比較するには、one-stage Fast R-CNNによってOverFeatシステムをエミュレートします（したがって、実装の詳細の他の違いを阻止します）。このシステムでは、「提案」 は3つのスケール（128、256、512）および3つのアスペクト比（1：1、1：2、2：1）のスライディングウィンドウです。Fast R-CNNはこれらのスライディングウィンドウからクラス分類のスコアと回帰ボックスの場所を予測するために訓練された。OverFeatシステムはイメージピラミッドを採用しているので、 5スケールから抽出された畳み込み特徴を使用して評価する。 表10は、Two-Stageと2つのOne-Stageの手法のバリアントを比較したものです 。 ZFモデルを使用して、 One-Stageの手法のmAPは53.9％です。 これは Two-Stageの手法（58.7％）よりも4.8％低い。 \n",
    "mAP:適合率の平均\n",
    "\n",
    "one-stage : クラスに特化した検出経路 一つのネットワークで領域抽出とカテゴリ識別を同時に行う方法で、一般的に高速に動作する。代表的なモデルとして、SSD,Yolo,Retina Net\n",
    "\n",
    "two-stage : クラスに依存しない提案とクラスに特化した検出からなる経路 物体が写っている領域の候補を検出するRPNと領域候補のカテゴリを識別するネットワークを直接に実行する方法。代表的なモデルはFaster R-CNN,Mask R-CNN\n",
    "\n",
    "*One-Stage Detection vs. Two-Stage Proposal + Detection.*  \n",
    "To compare the one-stage and two-stage systems, we emulate the OverFeat system (and thus also circumvent other differences of implementation details) by one-stage Fast R-CNN. In this system, the “proposals” are dense sliding windows of 3 scales (128, 256, 512) and 3 aspect ratios (1:1, 1:2, 2:1). Fast R-CNN is trained to predict class-specific scores and regress box locations from these sliding windows. Because the OverFeat system adopts an image pyramid, we also evaluate using convolutional features extracted from 5 scales. We use those 5 scales as in [1], [2]. Table 10 compares the two-stage system and two variants of the one-stage system. Using the ZF model, the one-stage system has an mAP of 53.9%. This is lower than the two-stage system (58.7%) by 4.8%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) RPNとは何か。\n",
    "RPNの構造の概要\n",
    "RPNは以下のような順序で実行されます。  \n",
    "①入力画像からfeature mapsを出力  \n",
    "②feature mapsに対してanchor boxesを設定する。  \n",
    "③anchor boxesと入力画像のラベル情報を元にoutput（RPNの後の層に渡すもの）を作成する  \n",
    "\n",
    "あるAnchor boxの中身が背景か物体か\n",
    "物体だった場合、ground truthとどのくらいズレているか\n",
    "の2つを学習させます。\n",
    "\n",
    "RPNは畳み込み層とプーリング層のみで記述をしています。一般的な分類問題では最後に全結合層が来るため、入力画像は固定長でなければいけませんが、RPNでは任意の大きさの画像を入力とすることが出来ます。そのため非常に柔軟性が高いモデルだと言える\n",
    "\n",
    "Anchorはfeature maps上の各点です。また、それぞれのAnchorに対して9つのAnchor boxesを作っていきます。そのAnchor boxesの情報がRPNのoutput\n",
    "\n",
    "In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection.\n",
    "\n",
    "BPNは、各位置でオブジェクトの境界とオブジェクトのスコアを同時に予測する完全な畳み込みネットワーク\n",
    "\n",
    "Firster　R-CNNが検出に使用する高品質のobject detectionを生成するためにエンドツーエンドでトレーニングされる\n",
    "\n",
    "物体の候補領域を見つけるAncherと呼ばれる領域（物体の領域を示している領域）を生成する\n",
    "\n",
    "候補の中からNMSなどを実施して、Region Proposal Networkでの物体の候補領域を抽出する NMS（Non-Maximum Suppression　　http://www.pyimagesearch.com/2014/11/17/non-maximum-suppression-object-detection-python/） とは重なりの多い領域を抑制し、領域の重複を防いでいる\n",
    "\n",
    "物体候補を出力するための2つの機能を持っています。1つ目は、枠内の画像が物体かどうかを表すスコアを計算する機能(cls layer)です。2つ目は、枠の概説矩形のスケールや位置を回帰により微調整する機能(reg layer)です。枠は、あらかじめ定義されたk個の外接矩形(Anchor)を用いて決定されます。このAnchor boxにさまざまな形、サイズを用意しておくとで多種多様な物体を検出できるようになるわけです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "物体検出：物体を取り囲む四角い領域を画像中から発見すること。\n",
    "物体領域候補を複数提案し、物体クラス認識の手法を用いてそれらの領域がどの物体に分類されるかを判断します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) RoIプーリングとは何か\n",
    "物体領域の幅、高さ、また次元数も異なるため後の違った次元数が入力できない全結層直接利用できないことから、次元数の異なる特徴マップを一定の大きさのベクトルに変換するRoiプーリングを使用する。\n",
    "\n",
    "任意サイズの領域をプーリングして、固定次元の出力をする 物体検出の領域は非常に可変長なので、得られた領域を次の分類ネットワークで処理するために、固定次元に縮小する処理を行う 不均一なサイズの入力に対して最大プールを実行して、固定サイズの特徴マップ（例えば14　* 14）を得ること。画像の多くの領域候補が必ず重複し、同じCNN計算を何度も実行していたので、画像を一回のみCNN実行し、領域間で計算結果を共有した 時間の大幅な短縮につながった\n",
    "\n",
    "RoIプーリングの目的は、この段階で7 × 7に設定されている任意のボックスから固定サイズの機能を作成することです。ボックスごとに、この機能に2つの追加の完全接続（fc）レイヤーを追加します。最初のfcレイヤー（ReLUを使用）は寸法を256に縮小し、次にピクセル単位のマスクを回帰する2番目のfcレイヤーが続きます。このマスクは、事前に定義された m × mの 空間解像度（m 28 を使用）であり、 m 2 次元ベクトルによってパラメーター化されます。2番目のfc層には m 2つの 出力。それぞれがグラウンドトゥルースマスクに対してバイナリロジスティック回帰を実行します。\n",
    "\n",
    "Shared computation of convolutions [9], [1], [29], [7], [2] has been attracting increasing attention for efficient, yet accurate, visual recognition. The OverFeat paper [9] computes convolutional features from an image pyramid for classification, localization, and detection. Adaptively-sized pooling (SPP) [1] on shared convolutional feature maps is developed for efficient region-based object detection [1], [30] and semantic segmentation [29]. Fast R-CNN [2] enables end-to-end detector training on shared convolutional features and shows compelling accuracy and speed.\n",
    "\n",
    "https://www.arxiv-vanity.com/papers/1512.04412/\n",
    "\n",
    "https://www.arxiv-vanity.com/papers/1703.06870/\n",
    "\n",
    "Given a box predicted by stage 1, we extract a feature of this box by Region-of-Interest (RoI) pooling [15, 9]. The purpose of RoI pooling is for producing a fixed-size feature from an arbitrary box, which is set as 14 × 14 at this stage. We append two extra fully-connected (fc) layers to this feature for each box. The first fc layer (with ReLU) reduces the dimension to 256, followed by the second fc layer that regresses a pixel-wise mask. This mask, of a pre-defined spatial resolution of m × m (we use m 28 ), is parameterized by an m 2 -dimensional vector. The second fc layer has m 2 outputs, each performing binary logistic regression to the ground truth mask.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) Anchorのサイズはどうするのが適切か。\n",
    "各sliding-windowで複数の領域提案を同時に予測する。各場所の可能な最大提案の数はkで表される。デフォルトでは　k = 9　でアンカーを生成する。そのkを中心としてアンカーボックスを生成し（k = 9ならば９つのアンカーボックス） 元画像の物体領域（ground truth boxes）\n",
    "さまざまな形、サイズを用意しておくとで多種多様な物体を検出できる（ただしボックスの面積を等しくなければならない）。\n",
    "\n",
    "At each sliding-window location, we simultaneously predict multiple region proposals, where the number of maximum possible proposals for each location is denoted as k. So the reg layer has 4k outputs encoding the coordinates of k boxes, and the cls layer outputs 2k scores that estimate probability of object or not object for each proposal4 . The k proposals are parameterized relative to k reference boxes, which we call　anchors. An anchor is centered at the sliding window in question, and is associated with a scale and aspect ratio (Figure 3, left). By default we use 3 scales and 3 aspect ratios, yielding k = 9 anchors at each sliding position. For a convolutional feature map of a size W × H (typically ∼2,400), there are W Hk anchors in total.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (7) 何というデータセットを使い、先行研究に比べどういった指標値が得られているか。\n",
    "COCOトレーニングセットを使用してトレーニングすると、Faster R-CNNはCOCOテスト開発セットで42.1％mAP@0.5 および21.5％mAP @ [.5、.95]を持ちます。これは2.8％高いmAP@0.5 および2.2％より高いのためのマップ@ [0.5、0.95]同じプロトコル（表の下の高速R-CNN対応物より XI）。これは、RPNが高いIoUしきい値でのローカライズの精度を向上させるために優れたパフォーマンスを発揮することを示しています。COCO trainvalセットを使用してトレーニングを行うと、Faster R-CNNはCOCO test-devセットで42.7％mAP@0.5 および21.9％mAP @ [.5、.95]になります。図 6は、MS COCO test-devセットのいくつかの結果を示しています。\n",
    "\n",
    "Next we evaluate our Faster R-CNN system. Using the COCO training set to train, Faster R-CNN has 42.1% mAP@0.5 and 21.5% mAP@[.5, .95] on the COCO test-dev set. This is 2.8% higher for mAP@0.5 and 2.2% higher for mAP@[.5, .95] than the Fast RCNN counterpart under the same protocol (Table 11). This indicates that RPN performs excellent for improving the localization accuracy at higher IoU thresholds. Using the COCO trainval set to train, Faster RCNN has 42.7% mAP@0.5 and 21.9% mAP@[.5, .95] on the COCO test-dev set. Figure 6 shows some results on the MS COCO test-dev set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
